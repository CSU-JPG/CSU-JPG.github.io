<!doctype html>
<html lang="en"> <!-- åˆå§‹åŒ–åä¼šéšæœ¬åœ°å­˜å‚¨åˆ‡æ¢ -->
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>People See Text, But LLM Not | CSU-JPG Lab Stories</title>
    <meta name="description" content="People see text visually; LLMs tokenize. Why this gap matters, what visual tokens change, and where vision-centric MLLMs are heading." />
    <meta property="og:title" content="People See Text, But LLM Not" />
    <meta property="og:description" content="People read visually, not symbolically. Visual tokens and vision-centric MLLMs point to the next paradigm." />
    <meta property="og:type" content="article" />
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='0.9em' font-size='90'>ğŸ§ </text></svg>">
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
      html{scroll-behavior:smooth}
      .prose p{line-height:1.8}
      .yt-wrap{position:relative;padding-top:56.25%}
      .yt-wrap iframe{position:absolute;inset:0;width:100%;height:100%;border:0;border-radius:12px}
      figure img{background:#fff}
      .lead{color:#525252}
      .prose {
        --tw-prose-body:#1f2937;--tw-prose-headings:#111827;--tw-prose-links:#2563eb;--tw-prose-bold:#111827;
        --tw-prose-quotes:#374151;--tw-prose-counters:#6b7280;--tw-prose-bullets:#6b7280;--tw-prose-hr:#e5e7eb;
        --tw-prose-code:#111827;--tw-prose-th-borders:#e5e7eb;--tw-prose-td-borders:#e5e7eb;
      }
      .prose h1,.prose h2,.prose h3,.prose h4,.prose h5,.prose h6{font-weight:800;line-height:1.25;text-transform:none}
      .prose h1{margin-top:.6em;margin-bottom:.3em}
      .prose h2{margin-top:2.2em;margin-bottom:.8em;padding-bottom:.3rem;border-bottom:1px solid #e5e7eb}
      .prose h3{margin-top:1.6em;margin-bottom:.6em}
      .prose a{color:#2563eb;text-decoration:none}
      .prose a:hover{text-decoration:underline;color:#1d4ed8}
      .prose blockquote{border-left:4px solid #e5e7eb;background:#f9fafb;color:#4b5563;padding:.75rem 1rem;border-radius:.5rem;margin:1rem 0;font-style:normal}
      .prose strong{font-weight:800;color:#111827}
      .prose :not(pre)>code{background:#f3f4f6;padding:.15rem .35rem;border-radius:.25rem;font-weight:500}
      .prose pre{background:#0b1220;color:#e5e7eb;border-radius:.6rem;padding:1rem;border:1px solid #111827;overflow:auto}
      .prose ul>li::marker,.prose ol>li::marker{color:#6b7280}
      .prose img{border-radius:.75rem;border:1px solid #e5e7eb;background:#fff}
      .prose figure figcaption{color:#6b7280;font-size:.78rem}
      #toc .font-semibold{color:#111827}
    </style>
  </head>
  <body class="bg-white text-neutral-900">
    <a id="top"></a>
    <header class="border-b border-neutral-200">
      <div class="max-w-4xl mx-auto px-4 py-4 flex items-center justify-between">
        <a id="homeLink" href="../" class="text-sm text-blue-700 hover:underline">â† Home</a>
        <div class="flex items-center gap-3">
          <div id="pubdate" class="text-sm text-neutral-500">Published on 2025-10-21 Â· CSU-JPG Lab</div>
          <div class="h-5 w-px bg-neutral-200"></div>
          <div class="text-sm">
            <button id="btnEN" class="px-2 py-1 rounded bg-black text-white">EN</button>
            <span class="mx-1 text-neutral-300">|</span>
            <button id="btnZH" class="px-2 py-1 rounded hover:bg-neutral-100">ä¸­æ–‡</button>
          </div>
        </div>
      </div>
    </header>

    <main class="max-w-4xl mx-auto px-4 py-10">
      <nav id="toc" class="mt-2 text-sm"></nav>

      <!-- ================= EN ================= -->
      <article id="content-en" data-lang="en" class="mt-6 prose prose-neutral md:prose-lg max-w-none">
        <h1 class="!mb-2">People See Text, But LLM Not</h1>
        <blockquote class="lead">
          â€œAoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are.â€ â€” Davis, Matt (2012)
        </blockquote>
        <p>You can probably read that sentence effortlessly. Despite the chaotic order of letters, your brain automatically reconstructs the intended words â€” because humans donâ€™t read text letter by letter. We perceive shapes, patterns, and visual words.</p>

        <h2>1. See Text vs. LLM Process Text</h2>

        <h3>1.1 The Visual Nature of Reading</h3>
        <p>Cognitive neuroscience has long shown that our brain recruits a specialized region called the <em>Visual Word Form Area</em> (VWFA) in the left occipitotemporal cortexâ€¦</p>

        <!-- (1) human_brain.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/human_brain.png" alt="Visual Word Form Area: reading is visual">
          <figcaption class="mt-2 text-xs text-neutral-500">The brain â€œseesâ€ words: reading is vision, not just language processing.</figcaption>
        </figure>

        <p><strong>In essence, people see text.</strong> Reading is vision â€” not just language processing.</p>

        <h3>1.2 How LLMs Process Text (and Why Itâ€™s Different)</h3>
        <p>LLMs tokenize text into subwords like â€œvis/ion/cent/ricâ€â€¦ the holistic form of text is lost in translation.</p>

        <h3>1.3 The Consequences of Tokenization</h3>
        <ul>
          <li><strong>Loss of visual semantics</strong> â€¦</li>
          <li><strong>Over-segmentation in multilingual text</strong> â€¦</li>
          <li><strong>Inefficiency for long text</strong> â€¦</li>
        </ul>

        <!-- (2) dvq.png (æ”¾åœ¨åˆ†è¯åæœç¤ºæ„) -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dvq.png" alt="Discrete tokenization vs. visual perception">
          <figcaption class="mt-2 text-xs text-neutral-500">Tokenization fragments sequences; visual perception keeps shapes/layout intact.</figcaption>
        </figure>

        <h3>1.4 Text Widely Exists in Web Images</h3>
        <p><strong>45%+</strong> web images contain text (e.g., LAION-2B; Lin et al., Parrot, ECCV 2024)â€¦</p>

        <!-- (3) parrot.png -->
        <figure class="mt-6">
          <img src="https://csu-jpg.github.io/Blog/figures/parrot.png" alt="Text prevalence in web images (Parrot)">
          <figcaption class="mt-2 text-xs text-neutral-500">Text is ubiquitous in web images: documents, UI, charts, designs.</figcaption>
        </figure>

        <h2>2. Early Attempts: Making Models See Text</h2>
        <p>Several early studies tried to bridge this gap by treating text as an image signal:</p>

        <!-- (4) pixel.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pixel.png" alt="PIXEL: render text as images, ViT-MAE pretraining">
          <figcaption class="mt-2 text-xs text-neutral-500">PIXEL (ICLRâ€™23): render text as images and pretrain with MAE.</figcaption>
        </figure>

        <!-- (5) clippo.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/clippo.png" alt="CLIPPO pixel-only encoder">
          <figcaption class="mt-2 text-xs text-neutral-500">CLIPPO (CVPRâ€™23): a pixel-only encoder for image and text.</figcaption>
        </figure>

        <!-- (6) pix2struct.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pix2struct.png" alt="Pix2Struct for screenshot and layout understanding">
          <figcaption class="mt-2 text-xs text-neutral-500">Pix2Struct (ICMLâ€™23): parse screenshots as structured visual input.</figcaption>
        </figure>

        <!-- (7) ptp.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/ptp.png" alt="PTP screenshot language models">
          <figcaption class="mt-2 text-xs text-neutral-500">PTP (arXivâ€™24): joint patch-and-text prediction on screenshots.</figcaption>
        </figure>

        <p>These works helped models see text, but left a key questionâ€¦</p>

        <h2>3. Recent Attempts: Visual Tokens for Long-Context Compression</h2>
        <p><strong>Key observations:</strong></p>
        <ul>
          <li>Vision encoders are smaller than LLMsâ€¦</li>
          <li>CLIP-style pretraining yields OCR-like abilitiesâ€¦</li>
          <li>Visual patches encode dense text â€” <em>spatial compression</em>.</li>
        </ul>

        <!-- (8) dense_text.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dense_text.png" alt="Dense text encoded by visual tokens">
          <figcaption class="mt-2 text-xs text-neutral-500">Dense textual content can be represented compactly via visual patches.</figcaption>
        </figure>

        <p>Interleaved document-level multimodal pretraining is idealâ€¦</p>

        <!-- (9) vis_in_context.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context.png" alt="Visual tokens for extended in-context learning">
          <figcaption class="mt-2 text-xs text-neutral-500">Leveraging visual tokens for extended in-context understanding.</figcaption>
        </figure>

        <!-- (10) vis_in_context_h100.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context_h100.png" alt="H100 pretraining: text context 256â†’2048">
          <figcaption class="mt-2 text-xs text-neutral-500">On NVIDIA H100: in-context text length improved from 256 â†’ 2048.</figcaption>
        </figure>

        <p>Long-text understanding in LLM is another ideal situationâ€¦</p>

        <!-- (11) vist.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vist.png" alt="VIST: vision-centric token compression in LLMs">
          <figcaption class="mt-2 text-xs text-neutral-500">VIST (NeurIPSâ€™25): fast visual path + slow language path.</figcaption>
        </figure>

        <p><strong>DeepSeek-OCR (Oct 2025)</strong> extends visual-token compression to OCRâ€¦</p>

        <!-- (12) deepseek_motivation.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/deepseek_motivation.png" alt="DeepSeek-OCR contextual optical compression">
          <figcaption class="mt-2 text-xs text-neutral-500">DeepSeek-OCR: ~97% precision with ~10Ã— compression.</figcaption>
        </figure>

        <h2>4. The Future Ahead: A Vision-centric MLLM</h2>
        <p>Tokenizer-free reading/generationâ€¦</p>

        <!-- (13) textatlas_5m.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/textatlas_5m.png" alt="TextAtlas5M dataset for dense text rendering">
          <figcaption class="mt-2 text-xs text-neutral-500">TextAtlas5M: large-scale dense text rendering.</figcaption>
        </figure>

        <!-- (14) long_text_ar.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/long_text_ar.png" alt="Beyond Words: long-text visual generation">
          <figcaption class="mt-2 text-xs text-neutral-500"><em>Beyond Words</em>: toward long-text visual generation.</figcaption>
        </figure>

        <h2>5. Toward the Next Generation of Vision-centric MLLM</h2>
        <ul>
          <li>Text is <strong>visual</strong>â€¦</li>
          <li>Vision is <strong>language</strong>â€¦</li>
          <li>Compression is <strong>perception</strong>â€¦</li>
        </ul>
        <p>The ultimate goalâ€¦ <strong>People see text. Soon, LLMs &amp; LVMs will too.</strong></p>
      </article>

      <!-- ================= ZH ================= -->
      <article id="content-zh" data-lang="zh" class="mt-6 prose prose-neutral md:prose-lg max-w-none" hidden>
        <h1 class="!mb-2">äººèƒ½â€œçœ‹è§â€æ–‡å­—ï¼Œä½† LLM å´ä¸èƒ½</h1>
        <blockquote class="lead">â€œAoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are.â€ â€” Davis, Matt (2012)</blockquote>
        <p>ä½ å¤§æ¦‚ç‡å¯ä»¥æ¯«ä¸è´¹åŠ›åœ°è¯»æ‡‚è¿™å¥è¯â€¦â€¦</p>

        <h2>1. äººâ€œçœ‹è§â€æ–‡æœ¬ vs. LLMâ€œå¤„ç†â€æ–‡æœ¬</h2>

        <h3>1.1 é˜…è¯»çš„è§†è§‰æœ¬è´¨</h3>
        <p>å¤§è„‘çš„ <em>è§†è§‰è¯å½¢åŒº</em>ï¼ˆVWFAï¼‰â€¦â€¦</p>

        <!-- (1) human_brain.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/human_brain.png" alt="è§†è§‰è¯å½¢åŒºç¤ºæ„">
          <figcaption class="mt-2 text-xs text-neutral-500">å¤§è„‘æ˜¯åœ¨â€œçœ‹â€æ–‡å­—ï¼šé˜…è¯»é¦–å…ˆæ˜¯è§†è§‰åŠ å·¥ã€‚</figcaption>
        </figure>

        <p><strong>æœ¬è´¨ä¸Šï¼Œäººç±»æ˜¯åœ¨â€œçœ‹â€æ–‡æœ¬ã€‚</strong></p>

        <h3>1.2 LLM å¦‚ä½•å¤„ç†æ–‡æœ¬</h3>
        <p>LLM å°†æ–‡æœ¬åˆ†è¯ä¸ºå­è¯â€¦â€¦</p>

        <h3>1.3 åˆ†è¯å¸¦æ¥çš„åæœ</h3>
        <ul>
          <li><strong>è§†è§‰è¯­ä¹‰ä¸¢å¤±</strong> â€¦</li>
          <li><strong>å¤šè¯­æ–‡æœ¬è¿‡åº¦åˆ‡åˆ†</strong> â€¦</li>
          <li><strong>é•¿æ–‡æœ¬ä½æ•ˆ</strong> â€¦</li>
        </ul>

        <!-- (2) dvq.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dvq.png" alt="ç¦»æ•£åˆ†è¯ vs è§†è§‰æ„ŸçŸ¥">
          <figcaption class="mt-2 text-xs text-neutral-500">åˆ†è¯æ‰“ç¢åºåˆ—ï¼›è§†è§‰ä¿æŒå½¢çŠ¶ä¸å¸ƒå±€ã€‚</figcaption>
        </figure>

        <h3>1.4 äº’è”ç½‘ä¸Šçš„å›¾åƒä¸­å¹¿æ³›å­˜åœ¨æ–‡æœ¬</h3>
        <p>åœ¨ LAION-2B ç­‰æ•°æ®ä¸­ï¼Œ<strong>45%+</strong> çš„ç½‘é¡µå›¾åƒåŒ…å«æ–‡æœ¬â€¦â€¦</p>

        <!-- (3) parrot.png -->
        <figure class="mt-6">
          <img src="https://csu-jpg.github.io/Blog/figures/parrot.png" alt="ç½‘é¡µå›¾åƒä¸­æ–‡å­—çš„æ™®éæ€§ï¼ˆParrotï¼‰">
          <figcaption class="mt-2 text-xs text-neutral-500">æ–‡æ¡£ã€UIã€å›¾è¡¨ä¸è®¾è®¡ï¼Œæœ¬è´¨éƒ½æ˜¯â€œè§†è§‰åŒ–æ–‡æœ¬â€ã€‚</figcaption>
        </figure>

        <h2>2. æ—©æœŸå°è¯•ï¼šè®©æ¨¡å‹â€œçœ‹è§â€æ–‡æœ¬</h2>
        <p>ä¸€äº›å·¥ä½œæŠŠæ–‡æœ¬å½“ä½œå›¾åƒä¿¡å·æ¥å»ºæ¨¡ï¼š</p>

        <!-- (4) pixel.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pixel.png" alt="PIXELï¼šæ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒ + MAE é¢„è®­ç»ƒ">
          <figcaption class="mt-2 text-xs text-neutral-500">PIXELï¼ˆICLRâ€™23ï¼‰ï¼šå»æ‰ tokenizerï¼Œåƒç´ é‡å»ºé¢„è®­ç»ƒã€‚</figcaption>
        </figure>

        <!-- (5) clippo.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/clippo.png" alt="CLIPPOï¼šåƒç´ ç»Ÿä¸€ç¼–ç å™¨">
          <figcaption class="mt-2 text-xs text-neutral-500">CLIPPOï¼ˆCVPRâ€™23ï¼‰ï¼šåƒç´ ç»Ÿä¸€ç¼–ç å›¾åƒä¸æ–‡æœ¬ã€‚</figcaption>
        </figure>

        <!-- (6) pix2struct.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pix2struct.png" alt="Pix2Structï¼šæˆªå›¾/ç‰ˆå¼ç†è§£">
          <figcaption class="mt-2 text-xs text-neutral-500">Pix2Structï¼ˆICMLâ€™23ï¼‰ï¼šç»“æ„åŒ–è§†è§‰è¾“å…¥è§£ææˆªå›¾ã€‚</figcaption>
        </figure>

        <!-- (7) ptp.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/ptp.png" alt="PTPï¼šæˆªå›¾è¯­è¨€æ¨¡å‹">
          <figcaption class="mt-2 text-xs text-neutral-500">PTPï¼ˆarXivâ€™24ï¼‰ï¼šè¡¥ä¸ä¸æ–‡æœ¬çš„è”åˆé‡å»ºã€‚</figcaption>
        </figure>

        <p>è¿™äº›å·¥ä½œè®©æ¨¡å‹å¼€å§‹â€œçœ‹è§â€æ–‡æœ¬ï¼Œä½†é‡åŒ–æ”¶ç›Šä»éœ€å›ç­”â€¦â€¦</p>

        <h2>3. è¿‘æœŸè¿›å±•ï¼šè§†è§‰ token åšé•¿ä¸Šä¸‹æ–‡å‹ç¼©</h2>
        <p><strong>å…³é”®è§‚å¯Ÿï¼š</strong></p>
        <ul>
          <li>è§†è§‰ç¼–ç å™¨è¿œå°äº LLMâ€¦â€¦</li>
          <li>CLIP é¢„è®­ç»ƒæ¶Œç°ç±» OCR èƒ½åŠ›â€¦â€¦</li>
          <li>è§†è§‰ patch ç¼–ç é«˜å¯†åº¦æ–‡æœ¬ï¼Œ<em>ç©ºé—´å‹ç¼©</em>â€¦â€¦</li>
        </ul>

        <!-- (8) dense_text.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dense_text.png" alt="é«˜å¯†åº¦æ–‡æœ¬çš„è§†è§‰è¡¨ç¤º">
          <figcaption class="mt-2 text-xs text-neutral-500">è§†è§‰ patch èƒ½æŠŠå¯†é›†å­—ç¬¦ç´§å‡‘è¡¨è¾¾ã€‚</figcaption>
        </figure>

        <p>äº¤é”™å¼æ–‡æ¡£çº§å¤šæ¨¡æ€é¢„è®­ç»ƒæ˜¯ç†æƒ³è®¾å®šâ€¦â€¦</p>

        <!-- (9) vis_in_context.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context.png" alt="è§†è§‰ token æ‰©å±• in-context ç†è§£">
          <figcaption class="mt-2 text-xs text-neutral-500">ç”¨è§†è§‰ token æ‰©å±•ä¸Šä¸‹æ–‡ä¸å¯†åº¦ã€‚</figcaption>
        </figure>

        <!-- (10) vis_in_context_h100.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context_h100.png" alt="H100 é¢„è®­ç»ƒï¼š256â†’2048">
          <figcaption class="mt-2 text-xs text-neutral-500">NVIDIA H100ï¼šä¸Šä¸‹æ–‡é•¿åº¦ä» 256 æå‡è‡³ 2048ã€‚</figcaption>
        </figure>

        <p>é•¿æ–‡æœ¬ç†è§£çš„å¦ä¸€ç†æƒ³åœºæ™¯â€¦â€¦</p>

        <!-- (11) vist.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vist.png" alt="VISTï¼šè§†è§‰ä¸ºå…ˆçš„ token å‹ç¼©">
          <figcaption class="mt-2 text-xs text-neutral-500">VISTï¼ˆNeurIPSâ€™25ï¼‰ï¼šå¿«/æ…¢åŒé€šè·¯é˜…è¯»ã€‚</figcaption>
        </figure>

        <p><strong>DeepSeek-OCRï¼ˆ2025 å¹´ 10 æœˆï¼‰</strong> å°†è§†è§‰ token å‹ç¼©æ‰©å±•åˆ° OCRâ€¦â€¦</p>

        <!-- (12) deepseek_motivation.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/deepseek_motivation.png" alt="DeepSeek-OCR åŠ¨æœºä¸æ•ˆæœ">
          <figcaption class="mt-2 text-xs text-neutral-500">çº¦ 97% ç²¾åº¦ã€çº¦ 10Ã— å‹ç¼©ã€‚</figcaption>
        </figure>

        <h2>4. æœªæ¥å›¾æ™¯ï¼šVision-centric MLLM</h2>
        <p>æˆ–è®¸ä¸å†éœ€è¦ä¼ ç»Ÿ tokenizerï¼›è§†è§‰åŒ–é˜…è¯»ä¸ç”Ÿæˆç»Ÿä¸€äºåŒä¸€ç©ºé—´â€¦â€¦</p>

        <!-- (13) textatlas_5m.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/textatlas_5m.png" alt="TextAtlas5M æ•°æ®é›†">
          <figcaption class="mt-2 text-xs text-neutral-500">TextAtlas5Mï¼šå¤§è§„æ¨¡è‡´å¯†æ–‡æœ¬æ¸²æŸ“ã€‚</figcaption>
        </figure>

        <!-- (14) long_text_ar.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/long_text_ar.png" alt="Beyond Wordsï¼šé•¿æ–‡æœ¬è§†è§‰ç”Ÿæˆ">
          <figcaption class="mt-2 text-xs text-neutral-500"><em>Beyond Words</em>ï¼šè¿ˆå‘çœŸæ­£çš„é•¿æ–‡æœ¬è§†è§‰ç”Ÿæˆã€‚</figcaption>
        </figure>

        <h2>5. èµ°å‘ä¸‹ä¸€ä»£ Vision-centric MLLM</h2>
        <ul>
          <li>æ–‡æœ¬é¦–å…ˆæ˜¯<strong>è§†è§‰</strong>â€¦</li>
          <li>è§†è§‰å°±æ˜¯<strong>è¯­è¨€</strong>â€¦</li>
          <li>å‹ç¼©å³<strong>æ„ŸçŸ¥</strong>â€¦</li>
        </ul>
        <p>ç»ˆæç›®æ ‡â€¦ <strong>People see text. å¾ˆå¿«ï¼ŒLLMs ä¸ LVMs ä¹Ÿä¼šå¦‚æ­¤ã€‚</strong></p>
      </article>

      <hr class="my-10 border-neutral-200"/>
      <div class="flex items-center justify-between text-sm">
        <a id="backHome" href="../" class="text-blue-700 hover:underline">â† Back to Home</a>
        <a id="backTop" href="#top" class="text-neutral-500 hover:underline">Back to Top â†‘</a>
      </div>
    </main>

    <footer class="border-t border-neutral-200 py-10 text-center text-sm text-neutral-500">
      Â© 2025 CSU-JPG Lab Â· <a class="hover:underline" href="https://csu-jpg.github.io" target="_blank" rel="noreferrer">https://csu-jpg.github.io</a>
    </footer>

    <script>
      const LS_KEY='csujpg_lang';
      const getLang=()=>localStorage.getItem(LS_KEY)||'en';
      const setLang=(l)=>{localStorage.setItem(LS_KEY,l);document.documentElement.lang=(l==='zh'?'zh-CN':'en');};
      const $en=document.getElementById('content-en');
      const $zh=document.getElementById('content-zh');
      const $toc=document.getElementById('toc');
      const $btnEN=document.getElementById('btnEN');
      const $btnZH=document.getElementById('btnZH');
      const UI_TEXT={en:{home:'â† Home',pub:'Published on 2025-10-21 Â· CSU-JPG Lab',toc:'Outline',backHome:'â† Back to Home',backTop:'Back to Top â†‘'},zh:{home:'â† è¿”å›é¦–é¡µ',pub:'å‘å¸ƒäº 2025-10-21 Â· CSU-JPG Lab',toc:'ç›®å½•',backHome:'â† è¿”å›é¦–é¡µ',backTop:'å›åˆ°é¡¶éƒ¨ â†‘'}};
      function buildTOC(){
        if(!$toc)return;
        const active=!$en.hidden?$en:$zh;
        const hs=active.querySelectorAll('h1,h2,h3');
        const ol=document.createElement('ol');ol.className='list-decimal list-inside mt-2 space-y-1 text-blue-700';
        hs.forEach((h,i)=>{if(!h.id){h.id='sec-'+i+'-'+(h.textContent||'').trim().toLowerCase().replace(/\s+/g,'-').replace(/[^\w\-]/g,'');}
          const li=document.createElement('li');const a=document.createElement('a');a.href='#'+h.id;a.textContent=h.textContent;a.className='hover:underline';li.appendChild(a);ol.appendChild(li);});
        const lang=(!$en.hidden?'en':'zh');$toc.innerHTML='';
        const title=document.createElement('div');title.className='font-semibold';title.textContent=UI_TEXT[lang].toc;$toc.appendChild(title);$toc.appendChild(ol);
      }
      function applyLang(l){
        setLang(l);
        if(l==='zh'){$en.hidden=true;$zh.hidden=false;$btnEN.className='px-2 py-1 rounded hover:bg-neutral-100';$btnZH.className='px-2 py-1 rounded bg-black text-white';}
        else{$en.hidden=false;$zh.hidden=true;$btnEN.className='px-2 py-1 rounded bg-black text-white';$btnZH.className='px-2 py-1 rounded hover:bg-neutral-100';}
        const t=UI_TEXT[l];document.getElementById('homeLink').textContent=t.home;document.getElementById('pubdate').textContent=t.pub;document.getElementById('backHome').textContent=t.backHome;document.getElementById('backTop').textContent=t.backTop;buildTOC();
      }
      applyLang(getLang());
      $btnEN.addEventListener('click',()=>applyLang('en'));
      $btnZH.addEventListener('click',()=>applyLang('zh'));
    </script>
  </body>
</html>
