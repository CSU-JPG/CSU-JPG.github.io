<!doctype html>
<html lang="en"> <!-- ÂàùÂßãÂåñÂêé‰ºöÈöèÊú¨Âú∞Â≠òÂÇ®ÂàáÊç¢ -->
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>People See Text, But LLM Not | CSU-JPG Lab Stories</title>
    <meta name="description" content="People see text visually; LLMs tokenize. Why this gap matters, what visual tokens change, and where vision-centric MLLMs are heading." />
    <meta property="og:title" content="People See Text, But LLM Not" />
    <meta property="og:description" content="People read visually, not symbolically. Visual tokens and vision-centric MLLMs point to the next paradigm." />
    <meta property="og:type" content="article" />
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='0.9em' font-size='90'>üß†</text></svg>">
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
      html{scroll-behavior:smooth}
      .prose p{line-height:1.8}
      .yt-wrap{position:relative;padding-top:56.25%}
      .yt-wrap iframe{position:absolute;inset:0;width:100%;height:100%;border:0;border-radius:12px}
      figure img{background:#fff}
      .lead{color:#525252}
      .prose {
        --tw-prose-body:#1f2937;--tw-prose-headings:#111827;--tw-prose-links:#2563eb;--tw-prose-bold:#111827;
        --tw-prose-quotes:#374151;--tw-prose-counters:#6b7280;--tw-prose-bullets:#6b7280;--tw-prose-hr:#e5e7eb;
        --tw-prose-code:#111827;--tw-prose-th-borders:#e5e7eb;--tw-prose-td-borders:#e5e7eb;
      }
      .prose h1,.prose h2,.prose h3,.prose h4,.prose h5,.prose h6{font-weight:800;line-height:1.25;text-transform:none}
      .prose h1{margin-top:.6em;margin-bottom:.3em}
      .prose h2{margin-top:2.2em;margin-bottom:.8em;padding-bottom:.3rem;border-bottom:1px solid #e5e7eb}
      .prose h3{margin-top:1.6em;margin-bottom:.6em}
      .prose a{color:#2563eb;text-decoration:none}
      .prose a:hover{text-decoration:underline;color:#1d4ed8}
      .prose blockquote{border-left:4px solid #e5e7eb;background:#f9fafb;color:#4b5563;padding:.75rem 1rem;border-radius:.5rem;margin:1rem 0;font-style:normal}
      .prose strong{font-weight:800;color:#111827}
      .prose :not(pre)>code{background:#f3f4f6;padding:.15rem .35rem;border-radius:.25rem;font-weight:500}
      .prose pre{background:#0b1220;color:#e5e7eb;border-radius:.6rem;padding:1rem;border:1px solid #111827;overflow:auto}
      .prose ul>li::marker,.prose ol>li::marker{color:#6b7280}
      .prose img{border-radius:.75rem;border:1px solid #e5e7eb;background:#fff}
      .prose figure figcaption{color:#6b7280;font-size:.78rem}
      #toc .font-semibold{color:#111827}
    </style>
  </head>
  <body class="bg-white text-neutral-900">
    <a id="top"></a>
    <header class="border-b border-neutral-200">
      <div class="max-w-4xl mx-auto px-4 py-4 flex items-center justify-between">
        <a id="homeLink" href="../" class="text-sm text-blue-700 hover:underline">‚Üê Home</a>
        <div class="flex items-center gap-3">
          <div id="pubdate" class="text-sm text-neutral-500">Published on 2025-10-21 ¬∑ CSU-JPG Lab</div>
          <div class="h-5 w-px bg-neutral-200"></div>
          <div class="text-sm">
            <button id="btnEN" class="px-2 py-1 rounded bg-black text-white">EN</button>
            <span class="mx-1 text-neutral-300">|</span>
            <button id="btnZH" class="px-2 py-1 rounded hover:bg-neutral-100">‰∏≠Êñá</button>
          </div>
        </div>
      </div>
    </header>

    <main class="max-w-4xl mx-auto px-4 py-10">
      <nav id="toc" class="mt-2 text-sm"></nav>

      <!-- ================= EN ================= -->
      <article id="content-en" data-lang="en" class="mt-6 prose prose-neutral md:prose-lg max-w-none">
        <h1 class="!mb-2">People See Text, But LLM Not</h1>
        <blockquote class="lead">
          ‚ÄúAoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are.‚Äù ‚Äî Davis, Matt (2012)
        </blockquote>
        <p>You can probably read that sentence effortlessly. Despite the chaotic order of letters, your brain automatically reconstructs the intended words ‚Äî because humans don‚Äôt read text letter by letter. We perceive shapes, patterns, and visual words.</p>

        <h2>1. See Text vs. LLM Process Text</h2>

        <h3>1.1 The Visual Nature of Reading</h3>
        <p>Cognitive neuroscience has long shown that our brain recruits a specialized region called the <em>Visual Word Form Area</em> (VWFA) in the left occipitotemporal cortex‚Ä¶</p>

        <!-- (1) human_brain.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/human_brain.png" alt="Visual Word Form Area: reading is visual">
          <figcaption class="mt-2 text-xs text-neutral-500">The brain ‚Äúsees‚Äù words: reading is vision, not just language processing.</figcaption>
        </figure>

        <p><strong>In essence, people see text.</strong> Reading is vision ‚Äî not just language processing.</p>

        <h3>1.2 How LLMs Process Text (and Why It‚Äôs Different)</h3>
        <p>LLMs tokenize text into subwords like ‚Äúvis/ion/cent/ric‚Äù‚Ä¶ the holistic form of text is lost in translation.</p>

        <h3>1.3 The Consequences of Tokenization</h3>
        <ul>
          <li><strong>Loss of visual semantics</strong> ‚Ä¶</li>
          <li><strong>Over-segmentation in multilingual text</strong> ‚Ä¶</li>
          <li><strong>Inefficiency for long text</strong> ‚Ä¶</li>
        </ul>

        <!-- (2) dvq.png (ÊîæÂú®ÂàÜËØçÂêéÊûúÁ§∫ÊÑè) -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dvq.png" alt="Discrete tokenization vs. visual perception">
          <figcaption class="mt-2 text-xs text-neutral-500">Tokenization fragments sequences; visual perception keeps shapes/layout intact.</figcaption>
        </figure>

        <h3>1.4 Text Widely Exists in Web Images</h3>
        <p><strong>45%+</strong> web images contain text (e.g., LAION-2B; Lin et al., Parrot, ECCV 2024)‚Ä¶</p>

        <!-- (3) parrot.png -->
        <figure class="mt-6">
          <img src="https://csu-jpg.github.io/Blog/figures/parrot.png" alt="Text prevalence in web images (Parrot)">
          <figcaption class="mt-2 text-xs text-neutral-500">Text is ubiquitous in web images: documents, UI, charts, designs.</figcaption>
        </figure>

        <h2>2. Early Attempts: Making Models See Text</h2>
        <p>Several early studies tried to bridge this gap by treating text as an image signal:</p>

        <!-- (4) pixel.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pixel.png" alt="PIXEL: render text as images, ViT-MAE pretraining">
          <figcaption class="mt-2 text-xs text-neutral-500">PIXEL (ICLR‚Äô23): render text as images and pretrain with MAE.</figcaption>
        </figure>

        <!-- (5) clippo.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/clippo.png" alt="CLIPPO pixel-only encoder">
          <figcaption class="mt-2 text-xs text-neutral-500">CLIPPO (CVPR‚Äô23): a pixel-only encoder for image and text.</figcaption>
        </figure>

        <!-- (6) pix2struct.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pix2struct.png" alt="Pix2Struct for screenshot and layout understanding">
          <figcaption class="mt-2 text-xs text-neutral-500">Pix2Struct (ICML‚Äô23): parse screenshots as structured visual input.</figcaption>
        </figure>

        <!-- (7) ptp.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/ptp.png" alt="PTP screenshot language models">
          <figcaption class="mt-2 text-xs text-neutral-500">PTP (arXiv‚Äô24): joint patch-and-text prediction on screenshots.</figcaption>
        </figure>

        <p>These works helped models see text, but left a key question‚Ä¶</p>

        <h2>3. Recent Attempts: Visual Tokens for Long-Context Compression</h2>
        <p><strong>Key observations:</strong></p>
        <ul>
          <li>Vision encoders are smaller than LLMs‚Ä¶</li>
          <li>CLIP-style pretraining yields OCR-like abilities‚Ä¶</li>
          <li>Visual patches encode dense text ‚Äî <em>spatial compression</em>.</li>
        </ul>

        <!-- (8) dense_text.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dense_text.png" alt="Dense text encoded by visual tokens">
          <figcaption class="mt-2 text-xs text-neutral-500">Dense textual content can be represented compactly via visual patches.</figcaption>
        </figure>

        <p>Interleaved document-level multimodal pretraining is ideal‚Ä¶</p>

        <!-- (9) vis_in_context.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context.png" alt="Visual tokens for extended in-context learning">
          <figcaption class="mt-2 text-xs text-neutral-500">Leveraging visual tokens for extended in-context understanding.</figcaption>
        </figure>

        <!-- (10) vis_in_context_h100.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context_h100.png" alt="H100 pretraining: text context 256‚Üí2048">
          <figcaption class="mt-2 text-xs text-neutral-500">On NVIDIA H100: in-context text length improved from 256 ‚Üí 2048.</figcaption>
        </figure>

        <p>Long-text understanding in LLM is another ideal situation‚Ä¶</p>

        <!-- (11) vist.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vist.png" alt="VIST: vision-centric token compression in LLMs">
          <figcaption class="mt-2 text-xs text-neutral-500">VIST (NeurIPS‚Äô25): fast visual path + slow language path.</figcaption>
        </figure>

        <p><strong>DeepSeek-OCR (Oct 2025)</strong> extends visual-token compression to OCR‚Ä¶</p>

        <!-- (12) deepseek_motivation.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/deepseek_motivation.png" alt="DeepSeek-OCR contextual optical compression">
          <figcaption class="mt-2 text-xs text-neutral-500">DeepSeek-OCR: ~97% precision with ~10√ó compression.</figcaption>
        </figure>

        <h2>4. The Future Ahead: A Vision-centric MLLM</h2>
        <p>Tokenizer-free reading/generation‚Ä¶</p>

        <!-- (13) textatlas_5m.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/textatlas_5m.png" alt="TextAtlas5M dataset for dense text rendering">
          <figcaption class="mt-2 text-xs text-neutral-500">TextAtlas5M: large-scale dense text rendering.</figcaption>
        </figure>

        <!-- (14) long_text_ar.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/long_text_ar.png" alt="Beyond Words: long-text visual generation">
          <figcaption class="mt-2 text-xs text-neutral-500"><em>Beyond Words</em>: toward long-text visual generation.</figcaption>
        </figure>

        <h2>5. Toward the Next Generation of Vision-centric MLLM</h2>
        <ul>
          <li>Text is <strong>visual</strong>‚Ä¶</li>
          <li>Vision is <strong>language</strong>‚Ä¶</li>
          <li>Compression is <strong>perception</strong>‚Ä¶</li>
        </ul>
        <p>The ultimate goal‚Ä¶ <strong>People see text. Soon, LLMs &amp; LVMs will too.</strong></p>
      </article>

      <!-- ================= ZH ================= -->
      <article id="content-zh" data-lang="zh" class="mt-6 prose prose-neutral md:prose-lg max-w-none" hidden>
        <h1 class="!mb-2">‰∫∫ËÉΩ‚ÄúÁúãËßÅ‚ÄùÊñáÂ≠óÔºå‰ΩÜ LLM Âç¥‰∏çËÉΩ</h1>
        <blockquote class="lead">‚ÄúAoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are.‚Äù ‚Äî Davis, Matt (2012)</blockquote>
        <p>‰Ω†Â§ßÊ¶ÇÁéáÂèØ‰ª•ÊØ´‰∏çË¥πÂäõÂú∞ËØªÊáÇËøôÂè•ËØù‚Ä¶‚Ä¶</p>

        <h2>1. ‰∫∫‚ÄúÁúãËßÅ‚ÄùÊñáÊú¨ vs. LLM‚ÄúÂ§ÑÁêÜ‚ÄùÊñáÊú¨</h2>

        <h3>1.1 ÈòÖËØªÁöÑËßÜËßâÊú¨Ë¥®</h3>
        <p>Â§ßËÑëÁöÑ <em>ËßÜËßâËØçÂΩ¢Âå∫</em>ÔºàVWFAÔºâ‚Ä¶‚Ä¶</p>

        <!-- (1) human_brain.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/human_brain.png" alt="ËßÜËßâËØçÂΩ¢Âå∫Á§∫ÊÑè">
          <figcaption class="mt-2 text-xs text-neutral-500">Â§ßËÑëÊòØÂú®‚ÄúÁúã‚ÄùÊñáÂ≠óÔºöÈòÖËØªÈ¶ñÂÖàÊòØËßÜËßâÂä†Â∑•„ÄÇ</figcaption>
        </figure>

        <p><strong>Êú¨Ë¥®‰∏äÔºå‰∫∫Á±ªÊòØÂú®‚ÄúÁúã‚ÄùÊñáÊú¨„ÄÇ</strong></p>

        <h3>1.2 LLM Â¶Ç‰ΩïÂ§ÑÁêÜÊñáÊú¨</h3>
        <p>LLM Â∞ÜÊñáÊú¨ÂàÜËØç‰∏∫Â≠êËØç‚Ä¶‚Ä¶</p>

        <h3>1.3 ÂàÜËØçÂ∏¶Êù•ÁöÑÂêéÊûú</h3>
        <ul>
          <li><strong>ËßÜËßâËØ≠‰πâ‰∏¢Â§±</strong> ‚Ä¶</li>
          <li><strong>Â§öËØ≠ÊñáÊú¨ËøáÂ∫¶ÂàáÂàÜ</strong> ‚Ä¶</li>
          <li><strong>ÈïøÊñáÊú¨‰ΩéÊïà</strong> ‚Ä¶</li>
        </ul>

        <!-- (2) dvq.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dvq.png" alt="Á¶ªÊï£ÂàÜËØç vs ËßÜËßâÊÑüÁü•">
          <figcaption class="mt-2 text-xs text-neutral-500">ÂàÜËØçÊâìÁ¢éÂ∫èÂàóÔºõËßÜËßâ‰øùÊåÅÂΩ¢Áä∂‰∏éÂ∏ÉÂ±Ä„ÄÇ</figcaption>
        </figure>

        <h3>1.4 ‰∫íËÅîÁΩë‰∏äÁöÑÂõæÂÉè‰∏≠ÂπøÊ≥õÂ≠òÂú®ÊñáÊú¨</h3>
        <p>Âú® LAION-2B Á≠âÊï∞ÊçÆ‰∏≠Ôºå<strong>45%+</strong> ÁöÑÁΩëÈ°µÂõæÂÉèÂåÖÂê´ÊñáÊú¨‚Ä¶‚Ä¶</p>

        <!-- (3) parrot.png -->
        <figure class="mt-6">
          <img src="https://csu-jpg.github.io/Blog/figures/parrot.png" alt="ÁΩëÈ°µÂõæÂÉè‰∏≠ÊñáÂ≠óÁöÑÊôÆÈÅçÊÄßÔºàParrotÔºâ">
          <figcaption class="mt-2 text-xs text-neutral-500">ÊñáÊ°£„ÄÅUI„ÄÅÂõæË°®‰∏éËÆæËÆ°ÔºåÊú¨Ë¥®ÈÉΩÊòØ‚ÄúËßÜËßâÂåñÊñáÊú¨‚Äù„ÄÇ</figcaption>
        </figure>

        <h2>2. Êó©ÊúüÂ∞ùËØïÔºöËÆ©Ê®°Âûã‚ÄúÁúãËßÅ‚ÄùÊñáÊú¨</h2>
        <p>‰∏Ä‰∫õÂ∑•‰ΩúÊääÊñáÊú¨ÂΩì‰ΩúÂõæÂÉè‰ø°Âè∑Êù•Âª∫Ê®°Ôºö</p>

        <!-- (4) pixel.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pixel.png" alt="PIXELÔºöÊñáÊú¨Ê∏≤Êüì‰∏∫ÂõæÂÉè + MAE È¢ÑËÆ≠ÁªÉ">
          <figcaption class="mt-2 text-xs text-neutral-500">PIXELÔºàICLR‚Äô23ÔºâÔºöÂéªÊéâ tokenizerÔºåÂÉèÁ¥†ÈáçÂª∫È¢ÑËÆ≠ÁªÉ„ÄÇ</figcaption>
        </figure>

        <!-- (5) clippo.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/clippo.png" alt="CLIPPOÔºöÂÉèÁ¥†Áªü‰∏ÄÁºñÁ†ÅÂô®">
          <figcaption class="mt-2 text-xs text-neutral-500">CLIPPOÔºàCVPR‚Äô23ÔºâÔºöÂÉèÁ¥†Áªü‰∏ÄÁºñÁ†ÅÂõæÂÉè‰∏éÊñáÊú¨„ÄÇ</figcaption>
        </figure>

        <!-- (6) pix2struct.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pix2struct.png" alt="Pix2StructÔºöÊà™Âõæ/ÁâàÂºèÁêÜËß£">
          <figcaption class="mt-2 text-xs text-neutral-500">Pix2StructÔºàICML‚Äô23ÔºâÔºöÁªìÊûÑÂåñËßÜËßâËæìÂÖ•Ëß£ÊûêÊà™Âõæ„ÄÇ</figcaption>
        </figure>

        <!-- (7) ptp.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/ptp.png" alt="PTPÔºöÊà™ÂõæËØ≠Ë®ÄÊ®°Âûã">
          <figcaption class="mt-2 text-xs text-neutral-500">PTPÔºàarXiv‚Äô24ÔºâÔºöË°•‰∏Å‰∏éÊñáÊú¨ÁöÑËÅîÂêàÈáçÂª∫„ÄÇ</figcaption>
        </figure>

        <p>Ëøô‰∫õÂ∑•‰ΩúËÆ©Ê®°ÂûãÂºÄÂßã‚ÄúÁúãËßÅ‚ÄùÊñáÊú¨Ôºå‰ΩÜÈáèÂåñÊî∂Áõä‰ªçÈúÄÂõûÁ≠î‚Ä¶‚Ä¶</p>

        <h2>3. ËøëÊúüËøõÂ±ïÔºöËßÜËßâ token ÂÅöÈïø‰∏ä‰∏ãÊñáÂéãÁº©</h2>
        <p><strong>ÂÖ≥ÈîÆËßÇÂØüÔºö</strong></p>
        <ul>
          <li>ËßÜËßâÁºñÁ†ÅÂô®ËøúÂ∞è‰∫é LLM‚Ä¶‚Ä¶</li>
          <li>CLIP È¢ÑËÆ≠ÁªÉÊ∂åÁé∞Á±ª OCR ËÉΩÂäõ‚Ä¶‚Ä¶</li>
          <li>ËßÜËßâ patch ÁºñÁ†ÅÈ´òÂØÜÂ∫¶ÊñáÊú¨Ôºå<em>Á©∫Èó¥ÂéãÁº©</em>‚Ä¶‚Ä¶</li>
        </ul>

        <!-- (8) dense_text.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dense_text.png" alt="È´òÂØÜÂ∫¶ÊñáÊú¨ÁöÑËßÜËßâË°®Á§∫">
          <figcaption class="mt-2 text-xs text-neutral-500">ËßÜËßâ patch ËÉΩÊääÂØÜÈõÜÂ≠óÁ¨¶Á¥ßÂáëË°®Ëææ„ÄÇ</figcaption>
        </figure>

        <p>‰∫§ÈîôÂºèÊñáÊ°£Á∫ßÂ§öÊ®°ÊÄÅÈ¢ÑËÆ≠ÁªÉÊòØÁêÜÊÉ≥ËÆæÂÆö‚Ä¶‚Ä¶</p>

        <!-- (9) vis_in_context.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context.png" alt="ËßÜËßâ token Êâ©Â±ï in-context ÁêÜËß£">
          <figcaption class="mt-2 text-xs text-neutral-500">Áî®ËßÜËßâ token Êâ©Â±ï‰∏ä‰∏ãÊñá‰∏éÂØÜÂ∫¶„ÄÇ</figcaption>
        </figure>

        <!-- (10) vis_in_context_h100.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context_h100.png" alt="H100 È¢ÑËÆ≠ÁªÉÔºö256‚Üí2048">
          <figcaption class="mt-2 text-xs text-neutral-500">NVIDIA H100Ôºö‰∏ä‰∏ãÊñáÈïøÂ∫¶‰ªé 256 ÊèêÂçáËá≥ 2048„ÄÇ</figcaption>
        </figure>

        <p>ÈïøÊñáÊú¨ÁêÜËß£ÁöÑÂè¶‰∏ÄÁêÜÊÉ≥Âú∫ÊôØ‚Ä¶‚Ä¶</p>

        <!-- (11) vist.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vist.png" alt="VISTÔºöËßÜËßâ‰∏∫ÂÖàÁöÑ token ÂéãÁº©">
          <figcaption class="mt-2 text-xs text-neutral-500">VISTÔºàNeurIPS‚Äô25ÔºâÔºöÂø´/ÊÖ¢ÂèåÈÄöË∑ØÈòÖËØª„ÄÇ</figcaption>
        </figure>

        <p><strong>DeepSeek-OCRÔºà2025 Âπ¥ 10 ÊúàÔºâ</strong> Â∞ÜËßÜËßâ token ÂéãÁº©Êâ©Â±ïÂà∞ OCR‚Ä¶‚Ä¶</p>

        <!-- (12) deepseek_motivation.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/deepseek_motivation.png" alt="DeepSeek-OCR Âä®Êú∫‰∏éÊïàÊûú">
          <figcaption class="mt-2 text-xs text-neutral-500">Á∫¶ 97% Á≤æÂ∫¶„ÄÅÁ∫¶ 10√ó ÂéãÁº©„ÄÇ</figcaption>
        </figure>

        <h2>4. Êú™Êù•ÂõæÊôØÔºöVision-centric MLLM</h2>
        <p>ÊàñËÆ∏‰∏çÂÜçÈúÄË¶Å‰º†Áªü tokenizerÔºõËßÜËßâÂåñÈòÖËØª‰∏éÁîüÊàêÁªü‰∏Ä‰∫éÂêå‰∏ÄÁ©∫Èó¥‚Ä¶‚Ä¶</p>

        <!-- (13) textatlas_5m.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/textatlas_5m.png" alt="TextAtlas5M Êï∞ÊçÆÈõÜ">
          <figcaption class="mt-2 text-xs text-neutral-500">TextAtlas5MÔºöÂ§ßËßÑÊ®°Ëá¥ÂØÜÊñáÊú¨Ê∏≤Êüì„ÄÇ</figcaption>
        </figure>

        <!-- (14) long_text_ar.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/long_text_ar.png" alt="Beyond WordsÔºöÈïøÊñáÊú¨ËßÜËßâÁîüÊàê">
          <figcaption class="mt-2 text-xs text-neutral-500"><em>Beyond Words</em>ÔºöËøàÂêëÁúüÊ≠£ÁöÑÈïøÊñáÊú¨ËßÜËßâÁîüÊàê„ÄÇ</figcaption>
        </figure>

        <h2>5. Ëµ∞Âêë‰∏ã‰∏Ä‰ª£ Vision-centric MLLM</h2>
        <ul>
          <li>ÊñáÊú¨È¶ñÂÖàÊòØ<strong>ËßÜËßâ</strong>‚Ä¶</li>
          <li>ËßÜËßâÂ∞±ÊòØ<strong>ËØ≠Ë®Ä</strong>‚Ä¶</li>
          <li>ÂéãÁº©Âç≥<strong>ÊÑüÁü•</strong>‚Ä¶</li>
        </ul>
        <p>ÁªàÊûÅÁõÆÊ†á‚Ä¶ <strong>People see text. ÂæàÂø´ÔºåLLMs ‰∏é LVMs ‰πü‰ºöÂ¶ÇÊ≠§„ÄÇ</strong></p>
      </article>

      <hr class="my-10 border-neutral-200"/>
      <div class="flex items-center justify-between text-sm">
        <a id="backHome" href="../" class="text-blue-700 hover:underline">‚Üê Back to Home</a>
        <a id="backTop" href="#top" class="text-neutral-500 hover:underline">Back to Top ‚Üë</a>
      </div>
    </main>

    <footer class="border-t border-neutral-200 py-10 text-center text-sm text-neutral-500">
      ¬© 2025 CSU-JPG Lab ¬∑ <a class="hover:underline" href="https://csu-jpg.github.io" target="_blank" rel="noreferrer">https://csu-jpg.github.io</a>
    </footer>

    <script>
      const LS_KEY='csujpg_lang';
      const getLang=()=>localStorage.getItem(LS_KEY)||'en';
      const setLang=(l)=>{localStorage.setItem(LS_KEY,l);document.documentElement.lang=(l==='zh'?'zh-CN':'en');};
      const $en=document.getElementById('content-en');
      const $zh=document.getElementById('content-zh');
      const $toc=document.getElementById('toc');
      const $btnEN=document.getElementById('btnEN');
      const $btnZH=document.getElementById('btnZH');
      const UI_TEXT={en:{home:'‚Üê Home',pub:'Published on 2025-10-21 ¬∑ CSU-JPG Lab',toc:'Outline',backHome:'‚Üê Back to Home',backTop:'Back to Top ‚Üë'},zh:{home:'‚Üê ËøîÂõûÈ¶ñÈ°µ',pub:'ÂèëÂ∏É‰∫é 2025-10-21 ¬∑ CSU-JPG Lab',toc:'ÁõÆÂΩï',backHome:'‚Üê ËøîÂõûÈ¶ñÈ°µ',backTop:'ÂõûÂà∞È°∂ÈÉ® ‚Üë'}};
      function buildTOC(){
        if(!$toc)return;
        const active=!$en.hidden?$en:$zh;
        const hs=active.querySelectorAll('h1,h2,h3');
        const ol=document.createElement('ol');ol.className='list-decimal list-inside mt-2 space-y-1 text-blue-700';
        hs.forEach((h,i)=>{if(!h.id){h.id='sec-'+i+'-'+(h.textContent||'').trim().toLowerCase().replace(/\s+/g,'-').replace(/[^\w\-]/g,'');}
          const li=document.createElement('li');const a=document.createElement('a');a.href='#'+h.id;a.textContent=h.textContent;a.className='hover:underline';li.appendChild(a);ol.appendChild(li);});
        const lang=(!$en.hidden?'en':'zh');$toc.innerHTML='';
        const title=document.createElement('div');title.className='font-semibold';title.textContent=UI_TEXT[lang].toc;$toc.appendChild(title);$toc.appendChild(ol);
      }
      function applyLang(l){
        setLang(l);
        if(l==='zh'){$en.hidden=true;$zh.hidden=false;$btnEN.className='px-2 py-1 rounded hover:bg-neutral-100';$btnZH.className='px-2 py-1 rounded bg-black text-white';}
        else{$en.hidden=false;$zh.hidden=true;$btnEN.className='px-2 py-1 rounded bg-black text-white';$btnZH.className='px-2 py-1 rounded hover:bg-neutral-100';}
        const t=UI_TEXT[l];document.getElementById('homeLink').textContent=t.home;document.getElementById('pubdate').textContent=t.pub;document.getElementById('backHome').textContent=t.backHome;document.getElementById('backTop').textContent=t.backTop;buildTOC();
      }
      applyLang(getLang());
      $btnEN.addEventListener('click',()=>applyLang('en'));
      $btnZH.addEventListener('click',()=>applyLang('zh'));
    </script>
  </body>
</html>
