<!doctype html>
<html lang="en"> <!-- ÈªòËÆ§Ëã±Êñá -->
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Stories | Vision-centric MLLM Evolution</title>
    <meta name="description" content="CSU-JPG: Research story and evolution around Leveraging Visual Tokens (NeurIPS'24) and Vision-centric Token Compression (NeurIPS'25 Spotlight)." />
    <meta property="og:title" content="Stories | Vision-centric MLLM Evolution" />
    <meta property="og:description" content="From visualizing long text to vision-fast-scan + language-deep-read, our two-step route for Vision-centric MLLM." />
    <meta property="og:type" content="article" />
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='0.9em' font-size='90'>üß†</text></svg>">
    <script src="https://cdn.tailwindcss.com"></script>
    <style>html{scroll-behavior:smooth}</style>
  </head>
  <body class="bg-white text-neutral-900">
    <header class="border-b border-neutral-200">
      <div class="max-w-4xl mx-auto px-4 py-4 flex items-center justify-between">
        <a id="homeLink" href="https://csu-jpg.github.io/" class="text-sm text-blue-700 hover:underline">‚Üê Home</a>
        <div class="flex items-center gap-3">
          <div id="pubdate" class="text-sm text-neutral-500">Published on 2025-10-21 ¬∑ CSU-JPG Lab</div>
          <div class="h-5 w-px bg-neutral-200"></div>
          <div class="text-sm">
            <button id="btnEN" class="px-2 py-1 rounded bg-black text-white">EN</button>
            <span class="mx-1 text-neutral-300">|</span>
            <button id="btnZH" class="px-2 py-1 rounded hover:bg-neutral-100">‰∏≠Êñá</button>
          </div>
        </div>
      </div>
    </header>

    <main class="max-w-4xl mx-auto px-4 py-10">
      <h1 id="title" class="text-3xl md:text-4xl font-extrabold leading-tight">
        Vision-centric MLLM: The Technical Evolution
      </h1>
      <p id="subtitle" class="mt-3 text-neutral-600">
        This article tells our two-step route for advancing MLLMs with a <strong>visual-first</strong> approach:
        <span class="whitespace-nowrap">‚ë† NeurIPS'24: Leveraging Visual Tokens</span> and
        <span class="whitespace-nowrap">‚ë° NeurIPS'25 Spotlight: Vision-centric Token Compression</span>.
      </p>

      <!-- Hero Figure -->
      <figure class="mt-6">
        <img class="w-full rounded-2xl border" src="https://fingerrec.github.io/index_files/jinpeng/papers/ARXIV2024/motivation.png" alt="NeurIPS 2024 Visual Tokens Motivation"/>
        <figcaption id="fig1cap" class="mt-2 text-xs text-neutral-500">
          Fig.1: Visualize long text so the vision encoder handles layout & dense info, relieving LLM context pressure (NeurIPS'24).
        </figcaption>
      </figure>

      <!-- Outline -->
      <nav class="mt-8 text-sm">
        <div id="tocTitle" class="font-semibold">Outline</div>
        <ol class="list-decimal list-inside mt-2 space-y-1 text-blue-700">
          <li><a id="toc1" class="hover:underline" href="#motivation">Motivation: Why ‚Äúvisualize text‚Äù?</a></li>
          <li><a id="toc2" class="hover:underline" href="#nips24">NeurIPS'24: Leveraging Visual Tokens for Extended Text Contexts</a></li>
          <li><a id="toc3" class="hover:underline" href="#nips25">NeurIPS'25 Spotlight: Vision-centric Token Compression in LLM</a></li>
          <li><a id="toc4" class="hover:underline" href="#compare">Contrast & Evolution: How the two steps connect</a></li>
          <li><a id="toc5" class="hover:underline" href="#related">Related Evidence & References</a></li>
        </ol>
      </nav>

      <section id="motivation" class="mt-10">
        <h2 id="motivationTitle" class="text-2xl font-bold">Motivation: Why ‚Äúvisualize text‚Äù?</h2>
        <p id="motivationBody" class="mt-3 leading-7 text-neutral-700">
          In real scenarios, long text is often <strong>mixed-layout</strong>, with <strong>dense characters</strong> and <strong>cross-region relations</strong>.
          Processing it directly with an LLM consumes context budget and strains layout understanding and cross-paragraph integration.
          Vision encoders excel at <strong>spatial structure</strong>, <strong>layout</strong> and <strong>scanning</strong>:
          by rendering long text into images or visual tokens, the vision model can ‚Äúscan first‚Äù and pass only key information to the LLM, enabling
          <strong>longer effective context, higher efficiency, and more robust understanding</strong>.
        </p>
      </section>

      <section id="nips24" class="mt-10">
        <h2 id="n24Title" class="text-2xl font-bold">NeurIPS'24: Leveraging Visual Tokens for Extended Text Contexts</h2>
        <p id="n24Body" class="mt-3 leading-7 text-neutral-700">
          In 2024, we proposed to <strong>compress long text with a vision encoder</strong>:
          ‚Äúvisualize text‚Äù and encode it as visual tokens, then reason with an LLM. This significantly extends effective context and is
          friendly to complex layouts, tables and screenshot-like inputs.
        </p>
        <ul id="n24Bullets" class="mt-3 list-disc list-inside text-neutral-700 space-y-1">
          <li>Intuition: save <em>context budget</em>, stabilize <em>cross-region associations</em>.</li>
          <li>Typical scenarios: manuals/webpages/paper screenshots, cross-paragraph QA, retrieval & reasoning with complex layout.</li>
        </ul>
        <figure class="mt-4">
          <img class="w-full rounded-2xl border" src="https://fingerrec.github.io/index_files/jinpeng/papers/longtextar2025/main_ppl.png" alt="LongTextAR Figure"/>
          <figcaption id="fig2cap" class="mt-2 text-xs text-neutral-500">
            Fig.2: Works like LongTextAR supplement training/evaluation for dense text image generation/understanding.
          </figcaption>
        </figure>
        <div class="mt-3 text-sm text-blue-700 space-x-4">
          <a id="n24Paper" class="hover:underline" href="https://arxiv.org/abs/2406.02547" target="_blank" rel="noreferrer">Paper (arXiv:2406.02547)</a>
          <a id="n24Code" class="hover:underline" href="https://github.com/showlab/visincontext" target="_blank" rel="noreferrer">Code</a>
          <a id="n24Proj" class="hover:underline" href="https://fingerrec.github.io/visincontext/" target="_blank" rel="noreferrer">Project</a>
        </div>
      </section>

      <section id="nips25" class="mt-10">
        <h2 id="n25Title" class="text-2xl font-bold">NeurIPS'25 Spotlight: Vision-centric Token Compression in LLM</h2>
        <p id="n25Body" class="mt-3 leading-7 text-neutral-700">
          In 2025, we further propose a <strong>reading heuristic</strong>:
          humans <em>scan</em> unimportant content visually and <em>deep-read</em> key content with language.
          Accordingly, we <strong>delegate non-critical content to the vision encoder for compression</strong>,
          reserving precious LLM context for core parts‚Äîachieving lower latency and stronger long-text handling.
        </p>
        <ul id="n25Bullets" class="mt-3 list-disc list-inside text-neutral-700 space-y-1">
          <li>Vision: fast scanning, structure extraction, redundancy compression.</li>
          <li>Language: key details, logical reasoning, high-level planning.</li>
          <li>Outcome: comparable or better performance with equal/smaller context.</li>
        </ul>
        <div class="mt-3 text-sm text-blue-700 space-x-4">
          <a id="n25Paper" class="hover:underline" href="https://arxiv.org/abs/2502.00791" target="_blank" rel="noreferrer">Paper (preprint)</a>
        </div>
      </section>

      <section id="compare" class="mt-10">
        <h2 id="cmpTitle" class="text-2xl font-bold">Contrast & Evolution: How the two steps connect</h2>
        <div class="mt-3 grid md:grid-cols-2 gap-4 text-sm">
          <div class="rounded-2xl border p-4">
            <div id="cmpL" class="font-semibold">Step 1 (NeurIPS'24)</div>
            <ul id="cmpLBullets" class="mt-2 list-disc list-inside space-y-1 text-neutral-700">
              <li>Visualize text ‚Üí compress long context via visual tokens</li>
              <li>Strengthen layout & dense-text understanding</li>
              <li>Significantly reduce LLM context usage</li>
            </ul>
          </div>
          <div class="rounded-2xl border p-4">
            <div id="cmpR" class="font-semibold">Step 2 (NeurIPS'25 Spotlight)</div>
            <ul id="cmpRBullets" class="mt-2 list-disc list-inside space-y-1 text-neutral-700">
              <li>Reading heuristic: vision fast-scan, language deep-read</li>
              <li>Dynamic context budgeting, more efficient</li>
              <li>End-to-end task performance continues improving</li>
            </ul>
          </div>
        </div>
      </section>

      <section id="related" class="mt-10">
        <h2 id="relTitle" class="text-2xl font-bold">Related Evidence & References</h2>
        <p id="relBody" class="mt-3 leading-7 text-neutral-700">
          The ‚Äúvisualizing text‚Äù idea is supported by several parallel lines of work in the community:
        </p>
        <ul id="relList" class="mt-2 list-disc list-inside text-neutral-700 space-y-1">
          <li>Princeton: <em>Language Modelling with Pixels</em> (ICLR 2023)</li>
          <li>Google Research: <em>CLIPPO: Image-and-Language Understanding from Pixels Only</em> (CVPR 2023)</li>
          <li>Lee et al.: <em>Pix2Struct</em> (ICML 2023)</li>
          <li>Danqi Chen‚Äôs group: <em>Improving Language Understanding from Screenshots</em> (ArXiv 2024/02)</li>
        </ul>
        <p id="relMore" class="mt-3 text-sm text-blue-700">
          Further reading:
          <a class="hover:underline" href="https://textatlas5m.github.io/" target="_blank" rel="noreferrer">TextAtlas5M</a> ,
          <a class="hover:underline" href="https://github.com/CSU-JPG/V-MAGE" target="_blank" rel="noreferrer">V-MAGE</a> ,
          <a class="hover:underline" href="https://github.com/CSU-JPG/MVPBench" target="_blank" rel="noreferrer">MVPBench</a>
        </p>
      </section>

      <hr class="my-10 border-neutral-200"/>
      <div class="flex items-center justify-between text-sm">
        <a id="backHome" href="./" class="text-blue-700 hover:underline">‚Üê Back to Home</a>
        <a id="backTop" href="#top" class="text-neutral-500 hover:underline">Back to Top ‚Üë</a>
      </div>
    </main>

    <footer class="border-t border-neutral-200 py-10 text-center text-sm text-neutral-500">
      ¬© 2025 CSU-JPG Lab ¬∑ <a class="hover:underline" href="https://csu-jpg.github.io" target="_blank" rel="noreferrer">https://csu-jpg.github.io</a>
    </footer>

    <script>
      // ===== ÁÆÄÊòì i18nÔºàÈªòËÆ§Ëã±ÊñáÔºâÔºå‰∏éÈ¶ñÈ°µ‰øùÊåÅ‰∏ÄËá¥ÁöÑ localStorage ÈîÆ =====
      const LS_KEY = 'csujpg_lang';
      const getLang = () => localStorage.getItem(LS_KEY) || 'en';
      const setLang = (l) => { localStorage.setItem(LS_KEY, l); document.documentElement.lang = (l==='en'?'en':'zh-CN'); };

      const T = {
        en: {
          home: '‚Üê Home',
          pubdate: 'Published on 2025-10-21 ¬∑ CSU-JPG Lab',
          title: 'Vision-centric MLLM: The Technical Evolution',
          subtitle: 'This article tells our two-step route for advancing MLLMs with a <strong>visual-first</strong> approach: <span class="whitespace-nowrap">‚ë† NeurIPS\'24: Leveraging Visual Tokens</span> and <span class="whitespace-nowrap">‚ë° NeurIPS\'25 Spotlight: Vision-centric Token Compression</span>.',
          fig1cap: 'Fig.1: Visualize long text so the vision encoder handles layout & dense info, relieving LLM context pressure (NeurIPS\'24).',
          tocTitle: 'Outline',
          toc1: 'Motivation: Why ‚Äúvisualize text‚Äù?',
          toc2: 'NeurIPS\'24: Leveraging Visual Tokens for Extended Text Contexts',
          toc3: 'NeurIPS\'25 Spotlight: Vision-centric Token Compression in LLM',
          toc4: 'Contrast & Evolution: How the two steps connect',
          toc5: 'Related Evidence & References',
          motivationTitle: 'Motivation: Why ‚Äúvisualize text‚Äù?',
          motivationBody: 'In real scenarios, long text is often <strong>mixed-layout</strong>, with <strong>dense characters</strong> and <strong>cross-region relations</strong>. Processing it directly with an LLM consumes context budget and strains layout understanding and cross-paragraph integration. Vision encoders excel at <strong>spatial structure</strong>, <strong>layout</strong> and <strong>scanning</strong>: by rendering long text into images or visual tokens, the vision model can ‚Äúscan first‚Äù and pass only key information to the LLM, enabling <strong>longer effective context, higher efficiency, and more robust understanding</strong>.',
          n24Title: 'NeurIPS\'24: Leveraging Visual Tokens for Extended Text Contexts',
          n24Body: 'In 2024, we proposed to <strong>compress long text with a vision encoder</strong>: ‚Äúvisualize text‚Äù and encode it as visual tokens, then reason with an LLM. This significantly extends effective context and is friendly to complex layouts, tables and screenshot-like inputs.',
          n24Bullets: [
            'Intuition: save <em>context budget</em>, stabilize <em>cross-region associations</em>.',
            'Typical scenarios: manuals/webpages/paper screenshots, cross-paragraph QA, retrieval & reasoning with complex layout.'
          ],
          fig2cap: 'Fig.2: Works like LongTextAR supplement training/evaluation for dense text image generation/understanding.',
          n24Paper: 'Paper (arXiv:2406.02547)',
          n24Code: 'Code',
          n24Proj: 'Project',
          n25Title: 'NeurIPS\'25 Spotlight: Vision-centric Token Compression in LLM',
          n25Body: 'In 2025, we further propose a <strong>reading heuristic</strong>: humans <em>scan</em> unimportant content visually and <em>deep-read</em> key content with language. Accordingly, we <strong>delegate non-critical content to the vision encoder for compression</strong>, reserving precious LLM context for core parts‚Äîachieving lower latency and stronger long-text handling.',
          n25Bullets: [
            'Vision: fast scanning, structure extraction, redundancy compression.',
            'Language: key details, logical reasoning, high-level planning.',
            'Outcome: comparable or better performance with equal/smaller context.'
          ],
          n25Paper: 'Paper (preprint)',
          cmpTitle: 'Contrast & Evolution: How the two steps connect',
          cmpL: 'Step 1 (NeurIPS\'24)',
          cmpLBullets: [
            'Visualize text ‚Üí compress long context via visual tokens',
            'Strengthen layout & dense-text understanding',
            'Significantly reduce LLM context usage'
          ],
          cmpR: 'Step 2 (NeurIPS\'25 Spotlight)',
          cmpRBullets: [
            'Reading heuristic: vision fast-scan, language deep-read',
            'Dynamic context budgeting, more efficient',
            'End-to-end task performance continues improving'
          ],
          relTitle: 'Related Evidence & References',
          relBody: 'The ‚Äúvisualizing text‚Äù idea is supported by several parallel lines of work in the community:',
          relList: [
            'Princeton: <em>Language Modelling with Pixels</em> (ICLR 2023)',
            'Google Research: <em>CLIPPO: Image-and-Language Understanding from Pixels Only</em> (CVPR 2023)',
            'Lee et al.: <em>Pix2Struct</em> (ICML 2023)',
            'Danqi Chen‚Äôs group: <em>Improving Language Understanding from Screenshots</em> (ArXiv 2024/02)'
          ],
          relMore: 'Further reading:',
          backHome: '‚Üê Back to Home',
          backTop: 'Back to Top ‚Üë'
        },
        zh: {
          home: '‚Üê ËøîÂõûÈ¶ñÈ°µ',
          pubdate: 'ÂèëÂ∏É‰∫é 2025-10-21 ¬∑ CSU-JPG Lab',
          title: '‰ª•ËßÜËßâ‰∏∫‰∏≠ÂøÉÔºàVision-centricÔºâÁöÑÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÊºîËøõ',
          subtitle: 'Êú¨ÊñáËÆ≤Ëø∞Êàë‰ª¨Â∞Ü<strong>ÈïøÊñáÊú¨‚ÄúËßÜËßâÂåñ‚Äù</strong>„ÄÅÂπ∂‰ª•<strong>ËßÜËßâ‰∏∫ÂÖà</strong>Êé®ËøõÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºàMLLMÔºâËÉΩÂäõÁöÑ‰∏§Ê≠•Ëµ∞Ë∑ØÁ∫øÔºö<span class="whitespace-nowrap">‚ë† NeurIPS\'24ÔºöLeveraging Visual Tokens</span> ‰∏é <span class="whitespace-nowrap">‚ë° NeurIPS\'25 SpotlightÔºöVision-centric Token Compression</span>„ÄÇ',
          fig1cap: 'Âõæ 1ÔºöÂ∞ÜÈïøÊñáÊú¨ËßÜËßâÂåñÔºåÁî±ËßÜËßâÁºñÁ†ÅÂô®Â§ÑÁêÜÂ∏ÉÂ±Ä‰∏éÂØÜÈõÜ‰ø°ÊÅØÔºåÁºìËß£ LLM ‰∏ä‰∏ãÊñáÂéãÂäõÔºàNeurIPS\'24Ôºâ„ÄÇ',
          tocTitle: 'ÁõÆÂΩï',
          toc1: 'Âä®Êú∫Ôºö‰∏∫‰Ωï‚ÄúÊääÊñáÊú¨ËßÜËßâÂåñ‚ÄùÔºü',
          toc2: 'NeurIPS\'24ÔºöLeveraging Visual Tokens for Extended Text Contexts',
          toc3: 'NeurIPS\'25 SpotlightÔºöVision-centric Token Compression in LLM',
          toc4: 'ÂØπÊØî‰∏éÊºîËøõÔºö‰∏§Ê≠•Ëµ∞Â¶Ç‰ΩïË°îÊé•',
          toc5: 'Áõ∏ÂÖ≥ËØÅÊçÆ‰∏éÂèÇËÄÉ',
          motivationTitle: 'Âä®Êú∫Ôºö‰∏∫‰Ωï‚ÄúÊääÊñáÊú¨ËßÜËßâÂåñ‚ÄùÔºü',
          motivationBody: 'Âú®ÁúüÂÆû‰∏ñÁïå‰∏≠ÔºåÈïøÊñáÊú¨ÂæÄÂæÄ<strong>Ê∑∑Êéí</strong>„ÄÅÂê´Êúâ<strong>Â∏ÉÂ±Ä</strong>„ÄÅ<strong>ÂØÜÈõÜÂ≠óÁ¨¶</strong>‰∏é<strong>Ë∑®Âå∫ÂüüÂÖ≥ËÅî</strong>„ÄÇÁõ¥Êé•ËÆ©ËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜËøô‰∫õËæìÂÖ•Ôºå‰∏ç‰ªÖÂç†Áî®ÂÆùË¥µÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£Ôºå‰πüËÆ©Ê®°ÂûãÂú®ÁâàÂºèÁêÜËß£‰∏éË∑®ÊÆµ‰ø°ÊÅØÊï¥Âêà‰∏äÊâøÂéã„ÄÇËßÜËßâÁºñÁ†ÅÂô®Âú®<strong>Á©∫Èó¥ÁªìÊûÑ</strong>„ÄÅ<strong>ÊéíÁâà</strong>‰∏é<strong>Êâ´Êèè</strong>ÊñπÈù¢ÂÖ∑Â§áÂ§©ÁÑ∂‰ºòÂäøÔºöÂ¶ÇÊûúÊàë‰ª¨ÂÖàÊääÈïøÊñáÊú¨Ê∏≤Êüì‰∏∫ÂõæÂÉèÊàñËßÜËßâ tokenÔºåËßÜËßâÊ®°ÂûãÂèØ‰ª•ÂÖàË°å‚ÄúÊâ´Âõæ‚ÄùÔºåÂè™ÊääÂÖ≥ÈîÆ‰ø°ÊÅØ‰º†ÈÄíÁªô LLMÔºå‰ªéËÄåÂÆûÁé∞<strong>Êõ¥Èïø‰∏ä‰∏ãÊñá„ÄÅÊõ¥È´òÊïàÁéá„ÄÅÊõ¥Á®≥ÂÅ•ÁöÑÁêÜËß£</strong>„ÄÇ',
          n24Title: 'NeurIPS\'24ÔºöLeveraging Visual Tokens for Extended Text Contexts',
          n24Body: 'Êàë‰ª¨Âú® 2024 Âπ¥ÊèêÂá∫Ôºö<strong>Áî®ËßÜËßâÁºñÁ†ÅÂô®ÂéãÁº©ÈïøÊñáÊú¨</strong>ÔºåÂ∞ÜÊñáÊú¨‚ÄúËßÜËßâÂåñ‚ÄùÂπ∂ÁºñÁ†Å‰∏∫ËßÜËßâ tokenÔºåÁªìÂêàËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÇËøô‰∏ÄÊñπÊ°àÊòæËëóÊãâÈïø‰∫ÜÊúâÊïà‰∏ä‰∏ãÊñáÔºåÂπ∂ÂØπÂ§çÊùÇÁâàÂºè„ÄÅË°®Ê†º„ÄÅÊà™ÂõæÁ±ªËæìÂÖ•Êõ¥ÂèãÂ•Ω„ÄÇ',
          n24Bullets: [
            'Áõ¥ËßÇÂ•ΩÂ§ÑÔºö<em>‰∏ä‰∏ãÊñáÈ¢ÑÁÆó</em>Êõ¥ÁúÅ„ÄÅ<em>Ë∑®Âå∫ÂüüÂÖ≥ËÅî</em>Êõ¥Á®≥„ÄÇ',
            'ÂÖ∏ÂûãÂú∫ÊôØÔºöËØ¥Êòé‰π¶/ÁΩëÈ°µ/ËÆ∫ÊñáÊà™ÂõæËß£Êûê„ÄÅË∑®ÊÆµËêΩÈóÆÁ≠î„ÄÅÂ∏¶Â§çÊùÇÊéíÁâàÁöÑÊ£ÄÁ¥¢‰∏éÊé®ÁêÜ„ÄÇ'
          ],
          fig2cap: 'Âõæ 2ÔºöLongTextAR Á≠âÁõ∏ÂÖ≥Â∑•‰ΩúË°•ÂÖÖ‰∫ÜÂØÜÈõÜÊñáÊú¨ÂõæÂÉèÁîüÊàê/ÁêÜËß£ÁöÑËÆ≠ÁªÉ‰∏éËØÑÊµãÁª¥Â∫¶„ÄÇ',
          n24Paper: 'ËÆ∫ÊñáÔºàarXiv:2406.02547Ôºâ',
          n24Code: '‰ª£Á†Å',
          n24Proj: 'È°πÁõÆÈ°µ',
          n25Title: 'NeurIPS\'25 SpotlightÔºöVision-centric Token Compression in LLM',
          n25Body: '2025 Âπ¥Êàë‰ª¨Ëøõ‰∏ÄÊ≠•ÊèêÂá∫<strong>ÈòÖËØªÂêØÂèëÂºè</strong>Ôºö<em>‰∫∫Á±ª‰ºöÁî®‚ÄúËßÜËßâ‚ÄùÂø´ÈÄüÊâ´Êèè‰∏çÈáçË¶Å‰ø°ÊÅØ„ÄÅÁî®‚ÄúËØ≠Ë®Ä‚ÄùÁ≤æËØªÂÖ≥ÈîÆ‰ø°ÊÅØ</em>„ÄÇÊçÆÊ≠§ÔºåÊàë‰ª¨Â∞Ü<strong>ÈùûÂÖ≥ÈîÆ‰ø°ÊÅØ‰∫§ÁªôËßÜËßâÁºñÁ†ÅÂô®ÂéãÁº©</strong>ÔºåÊääÂÆùË¥µÁöÑËØ≠Ë®Ä‰∏ä‰∏ãÊñáÁïôÁªôÊ†∏ÂøÉÂÜÖÂÆπÔºåÂÆûÁé∞Êõ¥‰ΩéÂª∂Ëøü„ÄÅÊõ¥Âº∫ÈïøÊñáÂ§ÑÁêÜËÉΩÂäõ„ÄÇ',
          n25Bullets: [
            'ËßÜËßâÔºöÂø´ÈÄüÊâ´Êèè„ÄÅÁªìÊûÑÊäΩÂèñ„ÄÅÂÜó‰ΩôÂéãÁº©„ÄÇ',
            'ËØ≠Ë®ÄÔºöÂÖ≥ÈîÆÁªÜËäÇ„ÄÅÈÄªËæëÊé®ÁêÜ„ÄÅÈ´òÂ±ÇËßÑÂàí„ÄÇ',
            'ÊïàÊûúÔºöÂú®ÂêåÁ≠âÊàñÊõ¥Â∞è‰∏ä‰∏ãÊñá‰∏ãÔºå‰øùÊåÅ/ÊèêÂçáÂ§çÊùÇ‰ªªÂä°ÊÄßËÉΩ„ÄÇ'
          ],
          n25Paper: 'ËÆ∫ÊñáÔºàÈ¢ÑÂç∞Êú¨Ôºâ',
          cmpTitle: 'ÂØπÊØî‰∏éÊºîËøõÔºö‰∏§Ê≠•Ëµ∞Â¶Ç‰ΩïË°îÊé•',
          cmpL: 'Step 1ÔºàNeurIPS\'24Ôºâ',
          cmpLBullets: [
            'ÊääÊñáÊú¨ËßÜËßâÂåñ ‚Üí ËßÜËßâ token ÂéãÁº©Èïø‰∏ä‰∏ãÊñá',
            'Âº∫ÂåñÂ∏ÉÂ±Ä‰∏éÂØÜÈõÜÊñáÊú¨ÁêÜËß£',
            'ÊòæËëóÈôç‰ΩéËØ≠Ë®Ä‰∏ä‰∏ãÊñáÂç†Áî®'
          ],
          cmpR: 'Step 2ÔºàNeurIPS\'25 SpotlightÔºâ',
          cmpRBullets: [
            'ÈòÖËØªÂêØÂèëÂºèÔºöËßÜËßâÂø´ÈÄüÊâ´„ÄÅËØ≠Ë®ÄÁ≤æËØª',
            'Âä®ÊÄÅÂàÜÈÖç‰∏ä‰∏ãÊñáÈ¢ÑÁÆóÔºåÊõ¥È´òÊïà',
            'Á´ØÂà∞Á´Ø‰ªªÂä°ÊÄßËÉΩÊåÅÁª≠ÊèêÂçá'
          ],
          relTitle: 'Áõ∏ÂÖ≥ËØÅÊçÆ‰∏éÂèÇËÄÉ',
          relBody: '‚ÄúÊñáÊú¨ËßÜËßâÂåñ‚ÄùÁöÑÊÄùË∑ØÂú®Á§æÂå∫‰∏≠ÊúâÂ§öÊù°Âπ≥Ë°åËØÅÊçÆÊîØÊíëÔºö',
          relList: [
            'PrincetonÔºö<em>Language Modelling with Pixels</em>ÔºàICLR 2023Ôºâ',
            'Google ResearchÔºö<em>CLIPPO: Image-and-Language Understanding from Pixels Only</em>ÔºàCVPR 2023Ôºâ',
            'Lee et al.Ôºö<em>Pix2Struct</em>ÔºàICML 2023Ôºâ',
            'Èôà‰∏πÂ•áÂõ¢ÈòüÔºö<em>Improving Language Understanding from Screenshots</em>ÔºàArXiv 2024/02Ôºâ'
          ],
          relMore: 'Âª∂‰º∏ÈòÖËØªÔºö',
          backHome: '‚Üê ËøîÂõûÈ¶ñÈ°µ',
          backTop: 'ÂõûÂà∞È°∂ÈÉ® ‚Üë'
        }
      };

      function applyLang(lang){
        const t = T[lang];
        // È°∂ÈÉ®
        document.getElementById('homeLink').textContent = t.home;
        document.getElementById('pubdate').textContent = t.pubdate;
        document.getElementById('btnEN').className = 'px-2 py-1 rounded ' + (lang==='en' ? 'bg-black text-white':'hover:bg-neutral-100');
        document.getElementById('btnZH').className = 'px-2 py-1 rounded ' + (lang==='zh' ? 'bg-black text-white':'hover:bg-neutral-100');

        // Ê†áÈ¢ò/ÂâØÊ†áÈ¢ò/ÂõæÊ≥®
        document.getElementById('title').textContent = '';
        document.getElementById('title').innerHTML = t.title;
        document.getElementById('subtitle').innerHTML = t.subtitle;
        document.getElementById('fig1cap').innerHTML = t.fig1cap;

        // ÁõÆÂΩï
        document.getElementById('tocTitle').textContent = t.tocTitle;
        document.getElementById('toc1').textContent = t.toc1;
        document.getElementById('toc2').textContent = t.toc2;
        document.getElementById('toc3').textContent = t.toc3;
        document.getElementById('toc4').textContent = t.toc4;
        document.getElementById('toc5').textContent = t.toc5;

        // Motivation
        document.getElementById('motivationTitle').textContent = t.motivationTitle;
        document.getElementById('motivationBody').innerHTML = t.motivationBody;

        // NeurIPS'24
        document.getElementById('n24Title').textContent = t.n24Title;
        document.getElementById('n24Body').innerHTML = t.n24Body;
        const n24B = document.getElementById('n24Bullets');
        n24B.innerHTML = '';
        t.n24Bullets.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; n24B.appendChild(el); });
        document.getElementById('fig2cap').innerHTML = t.fig2cap;
        document.getElementById('n24Paper').textContent = t.n24Paper;
        document.getElementById('n24Code').textContent = t.n24Code;
        document.getElementById('n24Proj').textContent = t.n24Proj;

        // NeurIPS'25
        document.getElementById('n25Title').textContent = t.n25Title;
        document.getElementById('n25Body').innerHTML = t.n25Body;
        const n25B = document.getElementById('n25Bullets');
        n25B.innerHTML = '';
        t.n25Bullets.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; n25B.appendChild(el); });
        document.getElementById('n25Paper').textContent = t.n25Paper;

        // ÂØπÊØî/ÊºîËøõ
        document.getElementById('cmpTitle').textContent = t.cmpTitle;
        document.getElementById('cmpL').textContent = t.cmpL;
        const cmpLB = document.getElementById('cmpLBullets');
        cmpLB.innerHTML = '';
        t.cmpLBullets.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; cmpLB.appendChild(el); });
        document.getElementById('cmpR').textContent = t.cmpR;
        const cmpRB = document.getElementById('cmpRBullets');
        cmpRB.innerHTML = '';
        t.cmpRBullets.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; cmpRB.appendChild(el); });

        // Áõ∏ÂÖ≥Â∑•‰Ωú
        document.getElementById('relTitle').textContent = t.relTitle;
        document.getElementById('relBody').innerHTML = t.relBody;
        const relList = document.getElementById('relList');
        relList.innerHTML = '';
        t.relList.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; relList.appendChild(el); });
        document.getElementById('relMore').firstChild.textContent = t.relMore + ' ';
        // Â∫ïÈÉ®ÊåâÈíÆ
        document.getElementById('backHome').textContent = t.backHome;
        document.getElementById('backTop').textContent = t.backTop;

        setLang(lang);
      }

      // ÂàùÂßãÂåñËØ≠Ë®Ä
      const langInit = getLang();
      setLang(langInit);
      applyLang(langInit);

      // ÂàáÊç¢‰∫ã‰ª∂
      document.getElementById('btnEN').addEventListener('click', ()=>applyLang('en'));
      document.getElementById('btnZH').addEventListener('click', ()=>applyLang('zh'));
    </script>
  </body>
</html>
